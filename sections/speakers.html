<div class="row me-row content-ct" id="speakers">
      <h2 class="row-title">Keynote Speakers</h2>
      <div class="row" id="lionelrobert">
        <div class="col-md-4 feature">
          <img src="img/wagner.jpg" class="keynote-img">
          <h3>Alan Richard Wagner</h3>
          <p>Pennsylvannia State University</p>
          <ul class="speaker-social">
            <li><a href="https://sites.psu.edu/real/"><span class="ti-world"></span></a></li>
          </ul>
        </div>
        <div class="col-md-8 feature">
          <h3>Understanding Human-Robot Trust during High-Risk Emergencies</h3>
          <p>

            This talk presents our ongoing effort to understand how and why people respond to faulty guidance from error prone robots during simulated high-risk emergencies.  We will present our recent attempts to understand and predict how people react to evacuation directions given by a robot during an emergency and how they calibrate their trust in the robot. Results from both in-person and virtual reality experiments provide evidence demonstrating that, in certain conditions, people will trust a robot too much, the role that anthropomorphism plays, and cues that predict who will overtrust a robot. Our results have implications for the use of robots in high-risk situations and foreshadow future issues that will need to be addressed. We also consider the ethical implications of creating emergency evacuation robots and present a set of ethical guidelines for developing evacuation robots. The talk will conclude by presenting avenues and motivations for future research.</p>
          <p></p>
        </div>
      </div>


      <div class="col-md-4 feature">
        <img src="img/karinne.jpg" class="keynote-img">
        <h3>Karinne Ramirez-Amaro</h3>
        <p>Chalmers University of Technology</p>
        <ul class="speaker-social">
          <li><a href="https://www.chalmers.se/en/persons/karinne/"><span class="ti-world"></span></a></li>
        </ul>
      </div>
      <div class="col-md-8 feature">
        <h3> Interpretable and Explainable AI meets Robotics - Robots that Learn and Reason from Experiences</h3>
        <p>
          The advances in Collaborative Robots (Cobots) have rapidly increased with the development of novel data- and knowledge-driven methods. These methods allow robots, to some extent, to explain their decisions. This research area is known as Explainable AI and is gaining importance in the robotics community. One advantage of such methods is the increase of human trust towards Cobots since robots could explain their decisions, especially when errors occur or when facing new situations. Interpretability and Explainability are challenging and important components when deploying Cobots into real and dynamic environments. In this talk, I will introduce a novel semantic-based learning method that generates compact and general models to infer human activities. I will also explain our current learning approaches to enable Cobots to learn from experience. Reasoning and learning from experiences are key when developing general-purpose machine learning methods. These experiences will allow robots to remember the best strategies to achieve a goal. Therefore, the new generation of robots should reason based on past experiences while providing explanations in case of errors. Thus, improving the autonomy of robots and humanâ€™s trust to work with robots.
          <p></p>
    </div>
      
    </div>